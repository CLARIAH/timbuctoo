= Changelog
:toc: macro
:toc-title: 
:toclevels: 4

NOTE: Apologies, we're a Dutch team and we used to comunicate primarily in Dutch. This list will be translated to English, but for now many parts are Dutch or a Dutch/English hybrid.

This document contains our todo list. It contains a list of ideas for improvements. It is a semi-automated dump from our issue tracker.

Each idea belongs to an Epic (a word derived from the fact that ideas are often named "user stories"). These epics might be derived from the code structure (i.e. a Java package is an Epic) or from a commitment we made to a funder (i.e. an Epic is a section in a proposal).

== The Epics

[[Tabular-data-import]]Tabular-data-import::
  Import non-rdf datasets (XML, CSV, Excel, MS Access) into a very simple RDF datamodel, and be able to specify how you would map that datamodel to an RDF graph.
[[Edit-GUI]]Edit-GUI::
  A GUI that will use the data descriptions to be able to edit arbitrary collections. We're aiming for something that allows a quick edit. It does not need to be customisable. (CLARIAH task 11.100)
[[Query-GUI]]Query-GUI::
  A gui for graph querying. The approach is that a user draws a pattern of the graph and the system answers what nodes match that pattern. note: 1. This is not an open-ended tool, but must have a clearly defined ceiling after which the user should switch to a true language such as SQL, SPARQL or an imperative language. 2. The tool will show what query it generates so that the user is put in contact with the query language. (3) The tool should provide queries based on the archetypes. (4) And you should be able to share your queries.
[[Faceted-search]]Faceted-search::
  Each dataset gets a faceted search based on the archetypes it specified. We're also building extension points to plug in more complex indexing configurations.
[[Discover-GUI]]Discover-GUI::
  A gui that is not aimed at precise querying, but rather to get a high-level overview of the data so that you know what queries to write.
[[RDF-import-export]]RDF-import-export::
  Being able to get your data from and to the LOD cloud and specifically from and to other timbuctoo instances. We might also implement a feature were a timbuctoo instance advertises it's datasets to other instances.
[[RDF-transform]]RDF-transform:: 
  Being able to project a graph as a new graph.
[[Dataset-interdependencies]]Dataset-interdependencies:: Be able to depend on other datasets.
  * Link to RDF subjects in a specific other dataset
  * Import (parts of) other datasets into your own dataset
  * Use RDF-transform to generate a new (read-only?) dataset that is fully dependent on other datasets
  * A GUI that allows you to see published datasets (with the author, institution etc.)
  * User management that specifies who is allowed to see your data
[[Dataset-search]]Dataset-search::
  * find subjects using their properties and then find the datasets that contain them
[[Data-curation-tools]]Data curation tools::
  Tools for cleaning up datasets. We're mostly implementing this by working together with existing tools.
[[Data-export]]Data-export::
  Render the timbuctoo data as RDF, graphviz and whatnot. This epic might also focus on generating data representations in a semantic form that can be well imported by other tools. (i.e. use the proper date-time encoding or geo-coordinates encoding)

There's also two epic's for stuff that is not immediately user visible:

[[Release-process]]Release process::
    Have a better release process
[[non-fatal-bugs]]non-fatal-bugs:: 
    Cleanup work, not really related to a specific task and not quite visible to the user, but needed to keep timbuctoo workeable.



toc::[]

== MVP
This version will allow people to see what timbuctoo does, and allow us to easily upload some datasets. It is not yet useable by 

[[TIM-453]]
=== [TIM-453] Add demo data^<<Tabular-data-import>>^

Context::
We hebben dalijk een mooie intro pagina, maar mensen willen kunnen zien wat er gebeurd met minimale moeite. Geef ze daarom een excel die ze kunnen downloaden die het goed gaat doen met onze importer. 

Definition of done::
Als iemand op de landing page komt kan hij op "download excel" klikken om een excel te downloaden. Die kan dan vervolgens het uplaod proces in. De excel bevat data die goed mapt naar onze abstracte types en die mooi showcased wat timbuctoo kan (losse kolommen als names mappen, links tussen objecten, VRE specifieke properties)

Development steps::
nadat we de importer hebben gebouwd pakken we een bestaande dataset zoals migrants, BIA of RAA en verknippen we die tot een niet al te groot excelletje met een overzichtelijke hoeveelheid kolommen

[[TIM-962]]
=== [TIM-962] een published dataset kunnen importeren^<<RDF-import-export>>^

Context::
Vanuit TIM-684 genereren we RDF files in dump en log formaat met een OAI-RS metadata file. In deze issue zorgen we dat timbuctoo die data kan importeren. Deze issue leunt op TIM-954 die een interface biedt om een quad naar de database te schrijven.

Definition of done::
Er is een (simpele bootstrap static) webpagina in timbuctoo waar je een url kan plakken. Timbuctoo zoekt dan via de resourcesync algoritmes naar de metadata xml en haalt daaruit de verschillende subgrafen op. Je kan kiezen welke je wil en vervolgens importeert timbuctoo die als nieuwe VRE.

Development steps::
1.  Doe <<TIM-954>>
2.  Implementeer het resourcesync discovery algoritme als java library (als top level directory in timbuctoo, of direct als losse repo)
3.  bouw een library die nquads en het patch formaat streamend kan parsen (als top level directory in timbuctoo, of direct als losse repo) er is zeker een library die je als basis kan gebruiken (1 seconde googlen leverde https://github.com/nxparser/nxparser op, wellicht is er een betere)
4.  bouw een functie die gegeven een url, de discovery functie aanroept, de files download en streamend parsed en importeert met de de library uit de vorige stap

=== [TIM-++] Hyperlinks kunnen aanleggen ^<<Dataset-interdependencies>>^
We willen kunnen zorgen dat we de urls van de subject via een same-as relatie kunnen aanlegen zodat je in de edit-interface in ieder geval kan naar de dbpedia/VIAF versie kan linken.

=== [TIM-++] Archetypes beter implementeren ^<<Edit-GUI>>^ ^<<RDF-import-export>>^
Als je in de mapper een archetype selecteert. Moet dit ervoor zorgen dat de edit interface ook echt dat datamodel toont (dus juist ook missende properties) zodat je als user het model herkent. Bonuspunten voor een GUI die echt toegespitst is op een archetype (bij personen rechtsboven groot de geboorte- en sterfdatum zien)

Velden van het model lostrekken van de extra velden (misschien handig om de mapping en de edit gui op elkaar te laten lijken wat dat betreft)

=== [TIM-++] Faceted search autogenereren op basis van de archetypes ^<<RDF-import-export>>^
Als tijdelijke shortcut kunnen we er eentje genereren voor de demo dataset.

=== [TIM-++] Meer 1 stijl maken
 - jouw stylesheet toepassen
 - gemockte onderdelen toevoegen
   - Een user environment (dit zijn je datasets, dit zijn je queries)

== Ready for data curation work

This version will refine the MVP until people who perform data-curation and storage as a job will find in timbuctoo a helpful tool.

[[TIM-1050]]
=== [TIM-1050] Verder kunnen werken aan een tussentijds opgeslagen import^<<Edit-GUI>>^

wanneer een MyVre van een gebruiker de status published=false heeft, moet de gebruiker er aan kunnen verder werken m.b.v. een GET op /v2.1/bulk-upload/MyVre/rml
hiertoe moet de RML response terugvertaald worden naar de frontend datastructuur.

[[TIM-999]]
=== [TIM-999] Be able to add new relation types^<<Tabular-data-import>>^

Each VRE usually adds some custom relation types. It would be nice to be able to add them during the import

[[TIM-996]]
=== [TIM-996] Make it possible to have multiple subtypes of an archetype in one VRE^<<Tabular-data-import>>^

Context::
Currently we assume that each archetype has one collection. This is an invariant that is not maintainable given the datasets that we have observed in the wild. Often a dataset will have multiple classes of collectives for example: Migrants vs Embassy employees which are very much distinct objects in the VRE. Even though the archetype concept is the same (they're all persons)
The only code that we know of that is dependent on this invariant is the relation edit interface. A relationtype is currently defined using the archetypes and the edit interface will find "the" implementation of that archetype in the VRE. 

[[TIM-951]]
=== [TIM-951] finialize import step?^<<Tabular-data-import>>^

 * Update the indexes 
 * generate PIDs when the import is finished

[[TIM-860]]
=== [TIM-860] Edit gui clears data after unsuccessful save new^<<Edit-GUI>>^


== Other...

All the other ideas are listed below. They have not yet been grouped into a logical increment.

[[TIM-1048]]
=== [TIM-1048] Displaynames ondersteunen^<<Tabular-data-import>>^

Context::
Je krijgt nu een url te zien als displayname. Het is fijner als de user kan kiezen welk veld de displayname bevat.

Definition of done::
This issue is considered delivered when:
1.  frontend laat de gebruiker aangeven wat de displayname wordt
2.  frontend maakt een predicateObjectMap voor rdfs:label aan voor de displayname
3.  de displayname van de VRE config gebruikt de rdfs:label property of de URI als die niet bestaat

[[TIM-1009]]
=== [TIM-1009] Make relationTypes work with unmapped RDF import^<<Tabular-data-import>>^

Context::
Currently relationTypes are bound to a sourceType and a targetType via a collection's archetype.
When importing RDF from a third party, however, we do not know which archetype to map a given collection to (if a collection is even present via rdf:type).
At present we opted to default an imported collection's archetype to the generic archetype collection concepts (a.k.a. things).
This results in relationType mappings containing sourceType=concept and targetType=concept.
The current Timbuctoo model makes the assumption that one archetype collection has no more than one inheriting collection per VRE, whereas in the unmapped RDF case this assumption is no longer valid. 

Impeding issues::

Mapping from one archetype to multiple collections

The most troublesome spot at the moment is the metadata description of a VRE (i.e. http://repository.huygens.knaw.nl/v2.1/metadata/WomenWriters). Especially this bit of code:

    // model/properties/JsonMetadata.java
    85.        Optional<String> targetType = abstractTargetType
    86.          .flatMap(typeName -> vre.getImplementerOf(typeName).map(Collection::getCollectionName));

When there is only one implementerOf the archetype there is no problem, but when there are multiple it will pick the first collection it finds.
To make it a little more concrete let us assume this model:

 importedCollectionA --> (hasArchetype) --> concepts
 importedCollectionB --> (hasArchetype) --> concepts
 
 importedEntityA <-- (hasEntity) <-- importedCollectionA
 importedEntityB <-- (hasEntity) <-- importedCollectionB

  importedEntityA --> (hasRelationTo) --> importedEntityB

When determining the sourceType and targetType the method getImplementerOf will always return importedCollectionA.
If I then want to create a new hasRelationTo relation via the Edit gui environment between something from importedCollectionA and importedCollectionB, I will not be able to, because the autocomplete will only show me suggestions from importedCollectionA.
A similar problem will occur in the Query GUI environment.

Directionality of the relationType

We may not assume that relations are symmetric from an RDF import, so the safe choice is to always create an outgoing relation type.
The fact that both the sourceType and the targetType inherit from the same archetype also makes the following code troublesome:

    // model/properties/JsonMetadata.java
    128.      .filter(v -> !getProp(v, "relationtype_sourceTypeName", String.class).orElse("").equals(abstractType))
    129.      .filter(v -> getProp(v, "relationtype_targetTypeName", String.class).orElse("").equals(abstractType))

The above code says: "give me all the relation types which are inbound" based on the names of the archeType, resulting in zero inbound relations. They all default to outbound. However, when presenting relations in the Edit GUI it becomes clear that all the relations which are actually inbound as per the imported RDF triples are not shown.
To make it a little more concrete let us assume this model:

  importedCollectionA --> (hasArchetype) --> concepts
  importedCollectionB --> (hasArchetype) --> concepts
 
  importedEntityA <-- (hasEntity) <-- importedCollectionA
  importedEntityB <-- (hasEntity) <-- importedCollectionB

  importedEntityA --> (hasRelationTo) --> importedEntityB

When showing importedEntityA, the outbound relation to importedEntityB is visible.
But, when showing importedEntityB, the inbound relation to importedEntityA is ignored, for the metadata endpoint now dictates:
    
    undesired

    {
      "importedCollectionAs": [{
        "name": "hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionAs/autocomplete",
        "relation": {
            "direction": "OUT",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionAs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }],
     "importedCollectionBs": [{
        "name": "hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionAs/autocomplete",
        "relation": {
            "direction": "OUT",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionAs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }]
    }

Assuming the imported RDF only has hasRelationTo predicates where the entity of type importedCollectionA is the subject and the entity of type importedCollectionB is the object:

    <foo:entityA> <rdf:type> <foo:collectionA> .
    <foo:entityB> <rdf:type> <foo:collectionB>
    <foo:entityA> <foo:hasRelationTo> <foo:entityB>

The desired response would be:
    
    desired

    {
      "importedCollectionAs": [{
        "name": "hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionBs/autocomplete",
        "relation": {
            "direction": "OUT",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionBs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }],
     "importedCollectionBs": [{
        "name": "inverse:hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionAs/autocomplete",
        "relation": {
            "direction": "IN",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionAs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }]
    }

Moreover, given the following situation:

    <foo:entityA> <rdf:type> <foo:collectionA> .
    <foo:entityB> <rdf:type> <foo:collectionB>
    <foo:entityC> <rdf:type> <foo:collectionA>
    <foo:entityD> <rdf:type> <foo:collectionB>

    <foo:entityA> <foo:hasRelationTo> <foo:entityB>
    <foo:entityA> <foo:hasRelationTo> <foo:entityC>
    <foo:entityD> <foo:hasRelationTo> <foo:entityB>

The desired response would be:

    desired2

    {
      "importedCollectionAs": [{
        "name": "hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionBs/autocomplete",
        "relation": {
            "direction": "OUT",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionBs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }, {
        "name": "hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionAs/autocomplete",
        "relation": {
            "direction": "OUT",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionAs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }, {
        "name": "inverse:hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionAs/autocomplete",
        "relation": {
            "direction": "IN",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollections",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }],
     "importedCollectionBs": [{
        "name": "hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionBs/autocomplete",
        "relation": {
            "direction": "OUT",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionBs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     },{
        "name": "inverse:hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionBs/autocomplete",
        "relation": {
            "direction": "IN",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionBs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }, {
        "name": "inverse:hasRelationTo",
        "type": "relation",
        "quicksearch": "/v2.1/domain/importedCollectionAs/autocomplete",
        "relation": {
            "direction": "IN",
            "outName": "hasRelationTo",
            "inName": "inverse:hasRelationTo",
            "targetCollection": "importedCollectionAs",
            "relationCollection": "importedrelations",
            "relationTypeId": "7a2ae7ed-57c5-4a4c-887a-e6df809ddc19"
        }
     }]
    }

This situation overloads some relation names and still creates some issues in the frontends: "did you mean hasRelationTo collectionA or hasRelationTo collectionB?"
However, this is more an issue of how to explain the situation to the end user than making the code function.
Resolving scenarios and their consequences
1. relationType definitions no longer inherit from an archetype and are only valid for a certain VRE
    *   The JSON metadata response will have to be told not to look for implementersOf but to directly return the collections which are mentioned in the relationType.
    *   All relationType definitions should now only be valid for the VRE they are requested for (add a hasRelationType edge between VRE and the relationType).
2. hybrid solution where relationTypes are either based on archetype or are the same as in the above scenario (as dictated by a new boolean property inheritsFromArchetype)
    *   Most quickly feasible (because no migration of existing relationType definitions is required) but might needlessly complicate the model.
3. generate a new archetype for every new imported collection
    *   The Admin VRE will become a jungle of archetype collections.
4. remove the source and target type restriction altogether
    *   This will drastically alter the model of Timbuctoo and have a big effect on the frontends.
    *   Only realistic when separating the code bases of Timbuctoo and Anansi (causing double the coding effort)
Only the first two scenarios seem even feasible for the MVP. Scenario 3 and 4 seem ill advised in any case.



[[TIM-1032]]
=== [TIM-1032] Be able add relations to collections outside your own VRE^<<Tabular-data-import>>^


It would be nice if you could link to other collections that you did not import. For example: if someone has a collection containing a well-curated list of languages. It would be nice to be able to re-use that through a link.
(Tabular-data-import because that's out main import currently and we'd probably need to at least add a GUI change here_

[[TIM-1046]]
=== [TIM-1046] names kunnen mappen^<<Tabular-data-import>>^

Context::
Het zou cool zijn als je ook de coole frontend names mapper kan gebruiken

* rml mapper moet blank nodes kunnen aanmaken
* frontend moet worden aangepast om names mapping rml doc te interpreteren
 * de crud service moet de rdf names ook goed tonen



[[TIM-1035]]
=== [TIM-1035] integratie met openrefine^<<Data-curation-tools>>^

Context::
Er is nogal wat curatie werk mogelijk. Ik zou die taken graag uitbesteden aan openrefine. Timbuctoo importeert de varieteit aan formaten naar rawvertices (een tabulair formaat in rdf) en gebruikt de openrefine API om daar een openrefine project voor aan te maken.
De mapping GUI kan dan een deel van zijn werk via openrefine doen (het deel wat nu niet in r2rml past) en een we kunnen de gebruiker ook langs openrefine leiden. Het resultaat (ook weer tabulair formaat) + de openrefine metadata (welke acties zijn uitgevoerd) slaan we dan weer in timbuctoo op.



[[TIM-994]]
=== [TIM-994] Design van Bas aan de frontend hangen^<<Tabular-data-import>>^


[[TIM-1004]]
=== [TIM-1004] Make rdf importer support multi-language text properties^<<RDF-import-export>>^


[[TIM-1005]]
=== [TIM-1005] Make rdf import support rdf datatypes^<<RDF-import-export>>^

Currently we throw away the rdf datatype information. We need to preserve it and to map the known datatypes to typed neo4j properties (so an integer becomes an integer in the database which means that you can do a graph query on everything less then 1)

[[TIM-787]]
=== [TIM-787] Add XML import^<<Tabular-data-import>>^

Needed among others for CKCC


[[TIM-325]]
=== [TIM-325] Be able to publish acceptance test result^<<Release-process>>^

[[TIM-725]]
=== [TIM-725] metrics server (graphite)^<<Release-process>>^


[[TIM-939]]
=== [TIM-939] Logging dashboard op monitor^<<Release-process>>^

Context::
Ik wil de health van de servers graag glanceable laten zien. Zowel voor ons (zodat we het zien als er iets down gaat) als voor bezoekers/Gertjan die langslopen (en die de supergoeie uptime kunnen zien)

extra constraint: de monitor moet niet 24 uur per dag draaien, maar ook niet aangezet hoeven worden. 24/7 is verspilling. aanzetten gaat geheid het klad in komen.

Definition of Done::
Er een monitor in de kamer staat die in een browser window de url van het graylog dashboard (later een ander dashboard) laat zien.

Development steps::

 *   Een raspberry pie automatisch een grafische omgeving en een full screen browser met als homepage de dashboard laten starten.
 *  aansluiten op jauco's netwerk poort. als het werkt een losse netwerk poort voor aanvragen.
 *  De memorychip op read only zetten.
 *  Een mechanische tijdschakelaar aan de monitor hangen en de raspberry pie via de usb van de monitor voeden. of de tijdschakelaar via een verdeeldoosje naar allebei laten wijzen.



[[TIM-694]]
=== [TIM-694] Start acceptence tests with a database with a controlled set of data^<<Release-process>>^

So we need to have mvn verify download a valid database
then we need to re-enable mvn verify on jenkins



[[TIM-682]]
=== [TIM-682] Query GUI: Variabelen in je query kunnen definieren^<<Query-GUI>>^


[[TIM-741]]
=== [TIM-741] AND/OR in query interface^<<Query-GUI>>^


[[TIM-927]]
=== [TIM-927] Dropdown is ugly^<<Edit-GUI>>^

For example if I open a dcarkeyword and I click on the type dropdown it looks to me like the list has 3 values. The values are the selected value, "subject" and "geography". 
I would like to see a better distinction between the selected value and the possible options, so the selected value does not look like a possible option.


 Comments  
 

Comment by Jauco Noordzij [ 21/Jun/16 ] 
En wat is je vraag? Of welke issue moet worden gefixed? en wat stel je voor? Martijn Maas


[[TIM-974]]
=== [TIM-974] Incorrect call to entityfetcher in TinkerpopJsonCrudService makes relation POST take 3-4 seconds (on the prod server)^<<non-fatal-bugs>>^

createRelation looks in the wrong collection for source and target.
It only works because the fallback GremlinEntityFetcher ignores the collectionName parameter.

146.    try {
147.      String collectionName = collection.getCollectionName(); // will return "wwrelations" when updating a WomenWriters relation
148.      Vertex sourceV = entityFetcher.getEntity(traversal, UUID.fromString(source.asText("")), null, collectionName)
149.                                    .next();
150.      try {
151.        Vertex targetV = entityFetcher.getEntity(traversal, UUID.fromString(target.asText("")), null, collectionName)
152.                                      .next();
153.        try {
154.          Vertex typeV = entityFetcher.getEntity(traversal, UUID.fromString(type.asText("")), null, collectionName)
155.                                      .next();
Debugging code output:

NO LUCENE INDEX FOR: wwrelations
NO LUCENE INDEX FOR: wwrelations
NO LUCENE INDEX FOR: wwrelations
FETCHING SOURCE TARGET AND TYPE TOOK 957ms



[[TIM-975]]
=== [TIM-975] Lack of lucene index for relation edges makes relation PUT take 2 seconds (on the prod server)^<<non-fatal-bugs>>^

This traversal is very slow on the server because edges do not yet have a lucene index.

721.      origEdge = graph.traversal().E()
722.        .has("tim_id", id.toString())
723.        .has("isLatest", true)
724.        .has("rev",  rev.intValue())
725.        .next();

LOOKUP of orig edge took: 789ms



[[TIM-955]]
=== [TIM-955] Remove CollectionBuilder.withDerivedRelations() and Collection.getDerivedRelations()^<<non-fatal-bugs>>^

This traversal is not generating any results. Also, the only configuration is not being used by the frontend (except via the search index), because language is a property of a written document. 
Therefore, the Collection.getDerivedRelations() method generated with the CollectionBuilder.withDerivedRelations() method should be removed.
—
proof.
Given this person:
http://acc.repository.huygens.knaw.nl/v2.1/domain/wwpersons/513a5609-ed03-4a1d-aec0-31602b5d9527
– there is no derived relation hasPersonLanguage in the response
However, following the isCreator of to this document:
http://acc.repository.huygens.knaw.nl/v2.1/domain/wwdocuments/f459fe83-1af2-48cd-ae25-f63fd89347f8
– there is a direct hasWorkLanguage relation 



[[TIM-519]]
=== [TIM-519] Foute POST naar /search/wwrelations/wwdocuments zonder otherSearchId geeft foutmelding: "sourceSearchId is not specified"^<<non-fatal-bugs>>^

moet zijn "otherSearchId is not specified"



[[TIM-947]]
=== [TIM-947] upgraden naar neo4j 3^<<non-fatal-bugs>>^


[[TIM-772]]
=== [TIM-772] Remove variations property from all vertices and edges^<<non-fatal-bugs>>^

The variationRefs in the json should be build from the types.



[[TIM-786]]
=== [TIM-786] Let maven ignore the database when copying the files from resources to target^<<non-fatal-bugs>>^


[[TIM-892]]
=== [TIM-892] TinkerpopJsonCrudService: herhaalde vertex/edge zoek logica naar losse methodes extraheren^<<non-fatal-bugs>>^


[[TIM-904]]
=== [TIM-904] TinkerpopJsonCrudService: C, R, U en D naar losse classe gooien^<<non-fatal-bugs>>^

dan spring je in ieder geval sneller naar de juiste plek. Is ook makkelijker te zien waar de gemeenschappelijke code zit en waar ze onafhankelijk van elkaar zijn.



[[TIM-902]]
=== [TIM-902] TinkerpopJsonCrudService: herhaalde edit logica van vertices/edges naar losse methode extraheren^<<non-fatal-bugs>>^

het gaat hier om de minimale properties die een vertex moet hebben



[[TIM-893]]
=== [TIM-893] neo4j-specifieke code encapsulaten^<<non-fatal-bugs>>^


[[TIM-732]]
=== [TIM-732] The config of the rpm is now easily overlooked when adding new required settings. Fix this.^<<non-fatal-bugs>>^


[[TIM-832]]
=== [TIM-832] Create changelog of data history^<<RDF-import-export>>^

Context::
We want to create a log for 2 reasons. The first is we want to simplify our datamodel. Now we want to connect our older versions of a Vertex with VERSION_OF relations. And we retrieve the latest version with a isLatest property set to true. This makes the code more difficult than it should be. To make it simpler we want to put all the old versions of Vertices and Edges into a changelog, so we only log the changes made in the latest revision. The second reason is, that we want to be able to generate a changelog in our REST API and for our RDF data export. We should be able to easily use this log function and export the data to RDF.

Definition of done::
1.  We have removed all the older versions of Edges and Vertices (with their VERSION_OF relations) and have their history recorded in a changelog in the database
2.  We are able to retrieve a changelog per Vertex through the REST API

Development steps::
1.  Be able to create a changelog of the current database (in a text file)
2.  save this changelog as vertices in the database
3.  Create this log in a DatabaseMigration
4.  Make sure the CRUD-actions create databaselog entries instead of the VERSION_OF duplicates
5.  Remove all older versions of the vertices and edges
6.  Remove the CRUD service and graphManager code that deals with VERSION_OF or isLatest
7.  Add a REST API to retrieve changes (http://repository.huygens.knaw.nl/v2.1/changelog?from=<token> of http://repository.huygens.knaw.nl/v2.1/changelog?from=start als je bij het begin wil beginnen.) De rest call geeft altijd maar 20 resultaten terug. Na die 20 heb je een token (laat de json ook maar een next link bevatten die dus from=<laatste-token> bevat). Dit vereist dus dat alle log items een id property krijgen en er een index voor is.



[[TIM-1007]]
=== [TIM-1007] GET of entity with many property configs takes minutes^<<non-fatal-bugs>>^

Context::
Import 119.777 entities from 1.000.000 triples in the dbpedia infoboxes set:
http://downloads.dbpedia.org/2015-10/core-i18n/nl/infobox_properties_nl.ttl.bz2
resulting database:
https://dl.dropboxusercontent.com/u/17090298/database-with-infoboxes.zip
And you get one collection with 4.903 property configurations.
This makes the union between one entity and its property converters take longer that it took to write this issue (still waiting for the response)
The gremlin query for the first entity only takes 15ms

g.V().hasLabel("collection").has("collectionName", "InfoBoxunknowns").out("hasEntityNode").out("hasEntity").limit(1)
Vertex [127033]:
  concept_naam: "La Victoria"
  created: "{\"timeStamp\":1469026401051,\"userId\":\"rdf-importer\"}"
  deleted: false
  InfoBoxunknown_bevolking: "1981"
  InfoBoxunknown_coatofarms: "70"
  InfoBoxunknown_dichtheid: "110"
  InfoBoxunknown_idnummer: "14065"
  InfoBoxunknown_latDeg: "37"
  InfoBoxunknown_latMin: "41"
  InfoBoxunknown_lonDeg: "4"
  InfoBoxunknown_lonDir: "W"
  InfoBoxunknown_lonMin: "51"
  InfoBoxunknown_naam: "La Victoria"
  InfoBoxunknown_oppervlaktotaal: "18"
  InfoBoxunknown_populatiedatum: "2007"
  isLatest: true
  modified: "{\"timeStamp\":1469026401051,\"userId\":\"rdf-importer\"}"
  rdfUri: "http://nl.dbpedia.org/resource/La_Victoria_(Córdoba)"
  rev: 1
  tim_id: "0dfa7202-2a07-4b3c-ad7a-ec068e57cb6a"
  types: "[\"InfoBoxunknown\",\"concept\"]"
  <--[hasEntity]-- v[390]
  <--[hasEntity]-- v[387]
  --[provincie]--> v[42096]
  --[regio]--> v[17251]
  --[munwebpage]--> v[127034]

Definition of done::
This issue is considered delivered when:
1.  Conversion of all the properties on the entity takes as much time as needed to convert only the properties the entity actually has (in this case 12, not 4.903)

Data model::
Currently the properties are transformed by doing a union between the traversals resulting from the property-configuration's converters and the entity.

    entityT.asAdmin().clone().union(propertyGetters).forEachRemaining(x -> {
      //Force side effects to happen
    });
Given 5000 propertyGetters this union takes forever (still waiting for the response of a single entity).

Development steps::
1.  For each entity vertex
2.  filter the property getters by the properties the entity actually has
3.  apply the filtered property getters on the entity vertex
finally
Response log: 772998 / 1000 / 60 = 12.8833 minutes

- INFO   < 200 GET http://localhost:8089/v2.1/domain/InfoBoxunknowns/0dfa7202-2a07-4b3c-ad7a-ec068e57cb6a (1577 bytes) (772998 ms) [n.k.h.t.logging.LoggingFilter]
De json

{

    "@type": "InfoBoxunknown",
    "_id": "0dfa7202-2a07-4b3c-ad7a-ec068e57cb6a",
    "naam": "La Victoria",
    "latDeg": "37",
    "latMin": "41",
    "lonDeg": "4",
    "lonMin": "51",
    "lonDir": "W",
    "dichtheid": "110",
    "bevolking": "1981",
    "coatofarms": "70",
    "populatiedatum": "2007",
    "idnummer": "14065",
    "oppervlaktotaal": "18",
    "@relationCount": 3,
    "@relations": {
        "provincie": [
            {
                "id": "a53e8c6f-03f9-4429-b622-6c93d5a2e482",
                "path": "domain/InfoBoxunknowns/a53e8c6f-03f9-4429-b622-6c93d5a2e482",
                "relationType": "provincie",
                "type": "InfoBoxunknown",
                "accepted": true,
                "relationId": "46d8b648-64ad-4476-be42-468fa3741fe5",
                "rev": 1,
                "displayName": "http://nl.dbpedia.org/resource/Córdoba_(provincie_van_Spanje)"
            }
        ],
        "munwebpage": [
            {
                "id": "199180ae-1136-4bf4-b3bd-d2fbb67103c7",
                "path": "domain/InfoBoxunknowns/199180ae-1136-4bf4-b3bd-d2fbb67103c7",
                "relationType": "munwebpage",
                "type": "InfoBoxunknown",
                "accepted": true,
                "relationId": "79649123-3f63-4300-8016-accfde44334a",
                "rev": 1,
                "displayName": "http://www.aytolavictoria.es/"
            }
        ],
        "regio": [
            {
                "id": "94ef883e-d8ab-4f69-9257-fa6543fe2e6a",
                "path": "domain/InfoBoxunknowns/94ef883e-d8ab-4f69-9257-fa6543fe2e6a",
                "relationType": "regio",
                "type": "InfoBoxunknown",
                "accepted": true,
                "relationId": "fabdb5a9-4d17-44c7-b622-8090c087dd10",
                "rev": 1,
                "displayName": "http://nl.dbpedia.org/resource/Andalusië"
            }
        ]
    },
    "^rev": 1,
    "^modified": {
        "timeStamp": 1469026401051,
        "userId": "rdf-importer"
    },
    "^created": {
        "timeStamp": 1469026401051,
        "userId": "rdf-importer"
    },
    "@variationRefs": [
        {
            "id": "0dfa7202-2a07-4b3c-ad7a-ec068e57cb6a",
            "type": "InfoBoxunknown"
        },
        {
            "id": "0dfa7202-2a07-4b3c-ad7a-ec068e57cb6a",
            "type": "concept"
        }
    ],
    "^deleted": false,
    "^pid": null
}

[[TIM-1056]]
=== [TIM-1056] De inversemappings testen fout^<<Edit-GUI>>^

[[TIM-1060]]
=== [TIM-1060] Op een snellere manier de properties kunnen mappen^<<Edit-GUI>>^

[[TIM-1063]]
=== [TIM-1063] properties saven onder hun predicate name (ipv alleen het laatste gedeelte ervan), en in de database als shortened predicates opslaan (tim:name)^<<Edit-GUI>>^

[[TIM-1062]]
=== [TIM-1062] RML kunnen loaden in de frontend^<<Edit-GUI>>^

[[TIM-1061]]
=== [TIM-1061] names kunnen mappen^<<Edit-GUI>>^
moet vooral code in de CRUD-get voor geschreven worden
